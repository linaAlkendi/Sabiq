import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import json

# 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df = pd.read_json("./backend/data/incidentData.json", encoding='utf-8')

# 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© (Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯Ø©)
if 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„' in df.columns:
    df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'] = pd.to_datetime(df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'])

# 3. Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù (Ù…Ø«Ø§Ù„: "2 Ø³Ø§Ø¹Ø©" -> Ø¯Ù‚Ø§Ø¦Ù‚)
def convert_duration(text):
    try:
        parts = text.strip().split()
        number = float(parts[0].replace(',', '.'))  # Ø¯Ø¹Ù… Ù„Ù„ÙØ§ØµÙ„Ø©
        unit = parts[1]
        if "Ø¯Ù‚ÙŠÙ‚Ø©" in unit or "Ø¯Ù‚Ø§ÙŠÙ‚" in unit:
            return number
        elif "Ø³Ø§Ø¹Ø©" in unit or "Ø³Ø§Ø¹Ø§Øª" in unit:
            return number * 60
        elif "ÙŠÙˆÙ…" in unit or "Ø§ÙŠØ§Ù…" in unit:
            return number * 1440
    except:
        return None

df['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'] = df['Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù'].apply(convert_duration)

# 4. ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ (Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„ØŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚ØŒ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©)
le_type = LabelEncoder()
le_facility = LabelEncoder()
le_severity = LabelEncoder()

df['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±'] = le_type.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'])
df['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'] = le_facility.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'])
df['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±'] = le_severity.fit_transform(df['Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©'])

# 5. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ø§Ù‚ØµØ© ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø©)
features = ['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±', 'Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚', 'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±'] 
df_clean = df.dropna(subset=features + ['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'])

X = df_clean[features]
y = df_clean['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±']

# 6. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 7. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le_facility.classes_))

# 8. Ø§Ù„ØªÙ†Ø¨Ø¤ - Ù…Ø«Ø§Ù„ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©)
example_type_encoded = le_type.transform(['ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ'])[0]  
example_duration = 120  # 120 Ø¯Ù‚ÙŠÙ‚Ø©
example_severity_encoded = le_severity.transform(['Ù…ØªÙˆØ³Ø·Ø©'])[0]

example_df = pd.DataFrame({
    'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±': [example_type_encoded],
    'Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚': [example_duration],
    'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±': [example_severity_encoded]
})

predicted_facility_encoded = model.predict(example_df)[0]
predicted_facility = le_facility.inverse_transform([predicted_facility_encoded])[0]

print(f"ğŸ”® Ø§Ù„ØªÙ†Ø¨Ø¤: Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø£Ù† ÙŠØ­ØµÙ„ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø±ÙÙ‚: {predicted_facility}")

# 9. ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ: Ø£ÙƒØ«Ø± Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¹Ø·Ù„ Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø±ÙÙ‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
common_causes_counts = (
    df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'] == predicted_facility]['Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©']
    .value_counts()
    .head(3)
    .to_dict()
)


# 10. ØªØ­Ø¶ÙŠØ± insights 
insights = [
    f"Ø£ØºÙ„Ø¨ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {predicted_facility} Ù‡ÙŠ {', '.join(common_causes_counts.keys())}.",
]

# Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø¹Ø·Ù„
avg_downtime = df_clean.groupby('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„')['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean()

mech_avg = avg_downtime.get('Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ', 0)
elec_avg = avg_downtime.get('ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ', 0)

if elec_avg > 0:
    diff_percent = ((mech_avg - elec_avg) / elec_avg) * 100
else:
    diff_percent = 0


if diff_percent > 0:
    insights.append(f"Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„Ù„Ø¹Ø·Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø£Ø¹Ù„Ù‰ Ø¨Ù†Ø³Ø¨Ø© {diff_percent:.0f}% Ù…Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ.")

# Ù„Ùˆ Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ÙƒØ§Ø¨Ù„Ø§Øª
if any('ÙƒØ§Ø¨Ù„' in cause for cause in common_causes_counts.keys()):
    insights.append("ÙŠÙ†ØµØ­ Ø¨Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¯ÙˆØ±ÙŠ Ù„Ù„ÙƒØ§Ø¨Ù„Ø§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„.")
else:
    insights.append("ØªÙˆØµÙ‰ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ©.")



# 11. ØªØ­Ø¶ÙŠØ± Ù†ØªÙŠØ¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù JSON
result = {
    "prediction": predicted_facility,
    "common_causes": common_causes_counts,
    "summary": df_clean.groupby('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„')['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean().round(2).to_dict(),
    "insights": insights
}

with open('./backend/data/output.json', 'w', encoding='utf-8') as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

print("ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù…Ù„Ù output.json")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import json
import datetime
from scipy.stats import linregress

# 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df = pd.read_json("./backend/data/incidentData.json", encoding='utf-8')

# 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'] = pd.to_datetime(df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'])

# 3. Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù (Ù…Ø«Ø§Ù„: "2 Ø³Ø§Ø¹Ø©" -> Ø¯Ù‚Ø§Ø¦Ù‚)
def convert_duration(text):
    try:
        parts = text.strip().split()
        number = float(parts[0].replace(',', '.'))  # Ø¯Ø¹Ù… Ù„Ù„ÙØ§ØµÙ„Ø©
        unit = parts[1]
        if "Ø¯Ù‚ÙŠÙ‚Ø©" in unit or "Ø¯Ù‚Ø§ÙŠÙ‚" in unit:
            return number
        elif "Ø³Ø§Ø¹Ø©" in unit or "Ø³Ø§Ø¹Ø§Øª" in unit:
            return number * 60
        elif "ÙŠÙˆÙ…" in unit or "Ø§ÙŠØ§Ù…" in unit:
            return number * 1440
    except:
        return None

df['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'] = df['Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù'].apply(convert_duration)

# 4. ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ (Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„ØŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚ØŒ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©)
le_type = LabelEncoder()
le_facility = LabelEncoder()
le_severity = LabelEncoder()

df['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±'] = le_type.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'])
df['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'] = le_facility.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'])
df['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±'] = le_severity.fit_transform(df['Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©'])

# 5. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ø§Ù‚ØµØ© ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø©)
features = ['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±', 'Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚', 'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±']
df_clean = df.dropna(subset=features + ['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'])

X = df_clean[features]
y = df_clean['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±']

# 6. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 7. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le_facility.classes_))

# 8. Ø§Ù„ØªÙ†Ø¨Ø¤ - Ù…Ø«Ø§Ù„ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©)
example_type_encoded = le_type.transform(['ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ'])[0]
example_duration = 120  # 120 Ø¯Ù‚ÙŠÙ‚Ø©
example_severity_encoded = le_severity.transform(['Ù…ØªÙˆØ³Ø·Ø©'])[0]

example_df = pd.DataFrame({
    'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±': [example_type_encoded],
    'Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚': [example_duration],
    'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±': [example_severity_encoded]
})

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
probs = model.predict_proba(example_df)
max_prob_index = probs.argmax(axis=1)[0]
max_prob = probs[0][max_prob_index]

predicted_facility = le_facility.inverse_transform([max_prob_index])[0]

print(f"ğŸ”® Ø§Ù„ØªÙ†Ø¨Ø¤: Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø£Ù† ÙŠØ­ØµÙ„ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø±ÙÙ‚: {predicted_facility} Ø¨Ù†Ø³Ø¨Ø© Ø§Ø­ØªÙ…Ø§Ù„ {max_prob*100:.1f}%")

# 9. ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ: Ø£ÙƒØ«Ø± Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¹Ø·Ù„ Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø±ÙÙ‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
common_causes_counts = (
    df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'] == predicted_facility]['Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©']
    .value_counts()
    .head(3)
    .to_dict()
)

# 10. Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø¹Ø·Ù„
avg_downtime = df_clean.groupby('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„')['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean()

mech_avg = avg_downtime.get('Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ', 0)
elec_avg = avg_downtime.get('ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ', 0)

insights = [
    f"Ø£ØºÙ„Ø¨ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {predicted_facility} Ù‡ÙŠ {', '.join(common_causes_counts.keys())}."
]

if elec_avg > 0:
    diff_percent = ((mech_avg - elec_avg) / elec_avg) * 100
else:
    diff_percent = 0

if diff_percent > 0:
    insights.append(f"Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„Ù„Ø¹Ø·Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø£Ø¹Ù„Ù‰ Ø¨Ù†Ø³Ø¨Ø© {diff_percent:.0f}% Ù…Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ.")

# 11. Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ Ø§Ù„Ø´Ù‡Ø±ÙŠ Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± 3 Ø£Ø´Ù‡Ø± Ù„Ù„Ù…Ø±ÙÙ‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
today = datetime.datetime.now()
period_start = today - pd.DateOffset(months=3)
recent_faults = df[df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'] >= period_start]

fault_counts = recent_faults['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'].value_counts()
monthly_fault_rates = fault_counts / 3
facility_rate = monthly_fault_rates.get(predicted_facility, 0)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø¥Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ ØªÙ‚Ø±ÙŠØ¨ÙŠ (ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ 0.25 Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©)
estimated_probability = min(facility_rate * 0.25, 1)

# 12. Ø­Ø³Ø§Ø¨ MTBF ÙˆØªÙˆÙ‚Ø¹ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù…
facility_faults_dates = df[df['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'] == predicted_facility]['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'].sort_values()

intervals = facility_faults_dates.diff().dt.days.dropna()

mtbf_days = intervals.mean()

last_fault_date = facility_faults_dates.max()

if pd.notna(mtbf_days):
    predicted_next_fault_date = last_fault_date + pd.Timedelta(days=mtbf_days)
else:
    predicted_next_fault_date = None

# 13. Ø¹Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ Ø´Ù‡Ø±ÙŠÙ‹Ø§ Ù„ÙƒÙ„ Ù…Ø±ÙÙ‚ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
monthly_counts = df.groupby([pd.Grouper(key='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„', freq='M'), 'Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚']).size().unstack().fillna(0)

facility_series = monthly_counts[predicted_facility]
x = range(len(facility_series))
slope, intercept, r_value, p_value, std_err = linregress(x, facility_series)

mean_faults = facility_series.mean()

if mean_faults > 0:
    percent_change = (slope / mean_faults) * 100
else:
    percent_change = 0

if slope > 0:
    insights.append(
        f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {predicted_facility} ÙÙŠ Ø§Ø±ØªÙØ§Ø¹ Ù…Ø³ØªÙ…Ø± Ø¨Ù…Ø¹Ø¯Ù„ Ø²ÙŠØ§Ø¯Ø© Ø´Ù‡Ø±ÙŠ ÙŠØ¨Ù„Øº {percent_change:.1f}% Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·."
    )
elif slope < 0:
    insights.append(
        f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {predicted_facility} ÙÙŠ Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø³ØªÙ…Ø± Ø¨Ù…Ø¹Ø¯Ù„ Ù†Ù‚ØµØ§Ù† Ø´Ù‡Ø±ÙŠ ÙŠØ¨Ù„Øº {abs(percent_change):.1f}% Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·."
    )
else:
    insights.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {predicted_facility} Ù…Ø³ØªÙ‚Ø± Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„Ù…Ø§Ø¶ÙŠØ©.")

# 14. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø­Ø¯ÙˆØ« Ø§Ù„Ø¹Ø·Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
if predicted_next_fault_date is not None:
    date_str = predicted_next_fault_date.strftime("%Y-%m-%d")
    insights.append(
        f"Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø£Ù† ÙŠØ­Ø¯Ø« Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… ÙÙŠ {predicted_facility} Ø­ÙˆØ§Ù„ÙŠ ÙŠÙˆÙ… {date_str} "
        f"Ø¨Ù…Ø¹Ø¯Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© {estimated_probability*100:.1f}% Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©."
    )
else:
    insights.append(
        f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ‚Ø¹ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… ÙÙŠ {predicted_facility}ØŒ "
        f"Ù„ÙƒÙ† Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø­Ø¯ÙˆØ« Ø¹Ø·Ù„ Ù‡ÙŠ {estimated_probability*100:.1f}%."
    )

# 15. ØªØ­Ø¶ÙŠØ± Ù†ØªÙŠØ¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù JSON
result = {
    "prediction": predicted_facility,
    "probability": f"{estimated_probability*100:.1f}%",
    "common_causes": common_causes_counts,
    "summary": df_clean.groupby('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„')['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean().round(2).to_dict(),
    "insights": insights
}

with open('./backend/data/output.json', 'w', encoding='utf-8') as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

print("ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù…Ù„Ù output.json")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import json
import datetime
from scipy.stats import linregress
import os
import joblib
import numpy as np

# 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_data():
    try:
        df = pd.read_json("./backend/data/incidentData.json", encoding='utf-8')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        return df
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
        return None

# 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
def convert_date_columns(df):
    try:
        df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'] = pd.to_datetime(df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'])
        print("âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨Ù†Ø¬Ø§Ø­")
        return df
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®: {str(e)}")
        return df

# 3. ØªØ­ÙˆÙŠÙ„ Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ø¥Ù„Ù‰ Ø¯Ù‚Ø§Ø¦Ù‚
def convert_duration(text):
    try:
        if pd.isna(text):
            return None
            
        parts = text.strip().split()
        number = float(parts[0].replace(',', '.'))  # Ø¯Ø¹Ù… Ù„Ù„ÙØ§ØµÙ„Ø©
        unit = parts[1]
        
        if "Ø¯Ù‚ÙŠÙ‚Ø©" in unit or "Ø¯Ù‚Ø§ÙŠÙ‚" in unit:
            return number
        elif "Ø³Ø§Ø¹Ø©" in unit or "Ø³Ø§Ø¹Ø§Øª" in unit:
            return number * 60
        elif "ÙŠÙˆÙ…" in unit or "Ø§ÙŠØ§Ù…" in unit:
            return number * 1440
        else:
            return None
    except:
        return None

def process_duration(df):
    df['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'] = df['Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù'].apply(convert_duration)
    print("âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ù…Ø¯Ø¯ Ø§Ù„ØªÙˆÙ‚Ù Ø¥Ù„Ù‰ Ø¯Ù‚Ø§Ø¦Ù‚")
    return df

# 4. ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ
def encode_categorical(df):
    try:
        le_type = LabelEncoder()
        le_facility = LabelEncoder()
        le_severity = LabelEncoder()

        df['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±'] = le_type.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'])
        df['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'] = le_facility.fit_transform(df['Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±ÙÙ‚'])
        df['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±'] = le_severity.fit_transform(df['Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©'])
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø±Ù…Ù‘Ø²Ø§Øª Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        joblib.dump(le_type, './backend/models/type_encoder.joblib')
        joblib.dump(le_facility, './backend/models/facility_encoder.joblib')
        joblib.dump(le_severity, './backend/models/severity_encoder.joblib')
        
        print("âœ… ØªÙ… ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
        return df, le_type, le_facility, le_severity
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ©: {str(e)}")
        return df, None, None, None

# 5. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def clean_data(df, features):
    try:
        df_clean = df.dropna(subset=features + ['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±'])
        print(f"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ: {len(df)}ØŒ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df_clean)}")
        return df_clean
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
        return df

# 6. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
def train_model(X, y):
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        y_pred = model.predict(X_test)
        print(classification_report(y_test, y_pred))
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        joblib.dump(model, './backend/models/fault_prediction_model.joblib')
        print("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­ÙØ¸Ù‡ Ø¨Ù†Ø¬Ø§Ø­")
        return model
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        return None

# 7. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ù…Ø±ÙÙ‚
def analyze_facilities(df_clean, le_facility):
    results = {}
    
    for facility_name in df_clean['Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙÙ‚'].unique():
        facility_df = df_clean[df_clean['Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙÙ‚'] == facility_name]

        if facility_df.empty or facility_df.shape[0] < 2:
            continue

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        common_causes_counts = (
            facility_df['Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©']
            .value_counts()
            .head(3)
            .to_dict()
        )

        # Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø¹Ø·Ù„
        avg_downtime = facility_df.groupby('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„')['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean()
        mech_avg = avg_downtime.get('Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ', 0)
        elec_avg = avg_downtime.get('ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ', 0)

        # ØªØ­Ù„ÙŠÙ„ MTBF (Mean Time Between Failures)
        facility_dates = facility_df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'].sort_values()
        intervals = facility_dates.diff().dt.days.dropna()
        mtbf_days = intervals.mean()
        last_fault_date = facility_dates.max()

        if pd.notna(mtbf_days):
            predicted_next_fault_date = last_fault_date + pd.Timedelta(days=mtbf_days)
        else:
            predicted_next_fault_date = None

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø²Ù…Ù†ÙŠ
        monthly_counts = df_clean.groupby(
            [pd.Grouper(key='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„', freq='M'), 'Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙÙ‚']
        ).size().unstack().fillna(0)
        
        if facility_name not in monthly_counts.columns:
            continue

        facility_series = monthly_counts[facility_name]
        x = range(len(facility_series))
        slope, _, _, _, _ = linregress(x, facility_series)

        mean_faults = facility_series.mean()
        if mean_faults > 0:
            percent_change = (slope / mean_faults) * 100
        else:
            percent_change = 0

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ù†ØµØ§Ø¦Ø­
        insights = []
        
        if common_causes_counts:
            insights.append(
                f"Ø£ØºÙ„Ø¨ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {facility_name} Ù‡ÙŠ {', '.join(common_causes_counts.keys())}."
            )

        if elec_avg > 0 and mech_avg > 0:
            diff_percent = ((mech_avg - elec_avg) / elec_avg) * 100
            if diff_percent > 0:
                insights.append(
                    f"Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„Ù„Ø¹Ø·Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø£Ø¹Ù„Ù‰ Ø¨Ù†Ø³Ø¨Ø© {diff_percent:.0f}% Ù…Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ."
                )

        if slope > 0:
            insights.append(
                f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {facility_name} ÙÙŠ Ø§Ø±ØªÙØ§Ø¹ Ù…Ø³ØªÙ…Ø± Ø¨Ù…Ø¹Ø¯Ù„ {percent_change:.1f}%."
            )
        elif slope < 0:
            insights.append(
                f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {facility_name} ÙÙŠ Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø³ØªÙ…Ø± Ø¨Ù…Ø¹Ø¯Ù„ {abs(percent_change):.1f}%."
            )
        else:
            insights.append(
                f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ ÙÙŠ {facility_name} Ù…Ø³ØªÙ‚Ø± Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„Ù…Ø§Ø¶ÙŠØ©."
            )

        # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø¹Ø·Ù„
        recent_faults = df_clean[
            df_clean['ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„'] >= datetime.datetime.now() - pd.DateOffset(months=3)
        ]
        fault_counts = recent_faults['Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙÙ‚'].value_counts()
        monthly_fault_rates = fault_counts / 3
        facility_rate = monthly_fault_rates.get(facility_name, 0)
        estimated_probability = min(facility_rate * 0.25, 1)

        if predicted_next_fault_date is not None:
            date_str = predicted_next_fault_date.strftime("%Y-%m-%d")
            insights.append(
                f"Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø£Ù† ÙŠØ­Ø¯Ø« Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… ÙÙŠ {facility_name} Ø­ÙˆØ§Ù„ÙŠ ÙŠÙˆÙ… {date_str} "
                f"Ø¨Ù…Ø¹Ø¯Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© {estimated_probability*100:.1f}% Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©."
            )
        else:
            insights.append(
                f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ‚Ø¹ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… ÙÙŠ {facility_name}ØŒ "
                f"Ù„ÙƒÙ† Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø­Ø¯ÙˆØ« Ø¹Ø·Ù„ Ù‡ÙŠ {estimated_probability*100:.1f}%."
            )

        results[facility_name] = {
            "probability": f"{estimated_probability*100:.1f}%",
            "common_causes": common_causes_counts,
            "summary": avg_downtime.round(2).to_dict(),
            "insights": insights,
            "next_predicted_date": predicted_next_fault_date.strftime("%Y-%m-%d") if predicted_next_fault_date else None,
            "mtbf_days": mtbf_days
        }
    
    return results

# 8. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
def generate_comprehensive_report(df_clean, facility_results):
    # ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù… Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„
    fault_type_analysis = {
        "ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ": {
            "count": len(df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ"]),
            "avg_downtime": df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ"]['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean(),
            "severity_distribution": df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ"]['Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©'].value_counts().to_dict()
        },
        "Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ": {
            "count": len(df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ"]),
            "avg_downtime": df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ"]['Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚'].mean(),
            "severity_distribution": df_clean[df_clean['Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø·Ù„'] == "Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ"]['Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©'].value_counts().to_dict()
        }
    }

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù…
    monthly_counts = df_clean.groupby(pd.Grouper(key='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø·Ù„', freq='M')).size()
    x = range(len(monthly_counts))
    slope, _, _, _, _ = linregress(x, monthly_counts)
    trend = "Ø²ÙŠØ§Ø¯Ø©" if slope > 0 else "Ø§Ù†Ø®ÙØ§Ø¶" if slope < 0 else "Ø«Ø¨Ø§Øª"

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø±ÙÙ‚ Ø§Ù„Ø£ÙƒØ«Ø± Ø¹Ø±Ø¶Ø© Ù„Ù„Ø£Ø¹Ø·Ø§Ù„
    top_facility = max(
        facility_results.items(),
        key=lambda x: float(x[1]['probability'][:-1])
    ) if facility_results else None

    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    report = {
        "predictions": list(facility_results.keys()),
        "top_prediction": {
            "facility": top_facility[0] if top_facility else None,
            "probability": top_facility[1]['probability'] if top_facility else None,
            "next_predicted_date": top_facility[1]['next_predicted_date'] if top_facility else None
        },
        "facility_analysis": facility_results,
        "fault_type_analysis": fault_type_analysis,
        "trend_analysis": {
            "trend": trend,
            "slope": abs(slope),
            "last_3_months": monthly_counts[-3:].tolist() if len(monthly_counts) >= 3 else None
        },
        "overall_insights": [
            f"Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø£Ø¹Ø·Ø§Ù„: {trend} Ø¨Ù…Ø¹Ø¯Ù„ {abs(slope):.2f} Ø¹Ø·Ù„ Ø´Ù‡Ø±ÙŠØ§Ù‹",
            f"Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„Ù„Ø¹Ø·Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ: {fault_type_analysis['ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ']['avg_downtime']:.1f} Ø¯Ù‚ÙŠÙ‚Ø©",
            f"Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù„Ù„Ø¹Ø·Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ: {fault_type_analysis['Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ']['avg_downtime']:.1f} Ø¯Ù‚ÙŠÙ‚Ø©"
        ]
    }

    return report

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¹Ø·Ø§Ù„...")
    
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = load_data()
    if df is None:
        return

    # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = convert_date_columns(df)
    df = process_duration(df)
    df, le_type, le_facility, le_severity = encode_categorical(df)
    
    # 3. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    features = ['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ø·Ù„_Ù…Ø´ÙØ±', 'Ù…Ø¯Ø©_Ø§Ù„ØªÙˆÙ‚Ù_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚', 'Ø¯Ø±Ø¬Ø©_Ø§Ù„Ø®Ø·ÙˆØ±Ø©_Ù…Ø´ÙØ±']
    df_clean = clean_data(df, features)
    
    # 4. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    X = df_clean[features]
    y = df_clean['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø±ÙÙ‚_Ù…Ø´ÙØ±']
    model = train_model(X, y)
    
    # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§ÙÙ‚
    facility_results = analyze_facilities(df_clean, le_facility)
    
    # 6. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_report = generate_comprehensive_report(df_clean, facility_results)
    
    # 7. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    try:
        os.makedirs("./backend/data", exist_ok=True)
        with open('./backend/data/output.json', 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2, default=str)
        print("âœ… ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù…Ù„Ù output.json")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {str(e)}")

if __name__ == "__main__":
    main()